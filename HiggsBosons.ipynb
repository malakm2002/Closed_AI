{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yNCR504X1Al"
      },
      "outputs": [],
      "source": [
        "# needed librairies\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, mean_squared_error\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "import xgboost as xgb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UKkmkmlO3v2"
      },
      "source": [
        "**Loading Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VltNZxjiO8n6",
        "outputId": "ada39544-9946-4f03-c7ca-049422b6c8e5"
      },
      "outputs": [],
      "source": [
        "# grant access to the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "\n",
        "# load the dataset located in a folder named \"ML\" that's in your root Google Drive\n",
        "data = pd.read_csv('/content/drive/MyDrive/ML/HIGGS_train.csv', dtype={'8': float, '21': float})\n",
        "\n",
        "# define the headers of the dataset\n",
        "column_names = ['class_label', 'lepton_pt', 'lepton_eta', 'lepton_phi', 'missing_energy_magnitude', 'missing_energy_phi',\n",
        "                'jet_1_pt', 'jet_1_eta', 'jet_1_phi', 'jet_1_btag', 'jet_2_pt', 'jet_2_eta', 'jet_2_phi', 'jet_2_btag',\n",
        "                'jet_3_pt', 'jet_3_eta', 'jet_3_phi', 'jet_3_btag', 'jet_4_pt', 'jet_4_eta', 'jet_4_phi', 'jet_4_btag',\n",
        "                'm_jj', 'm_jjj', 'm_lv', 'm_jlv', 'm_bb', 'm_wbb', 'm_wwbb']\n",
        "\n",
        "# assign the headers to the data\n",
        "data.columns=column_names\n",
        "\n",
        "cleaned_data = data.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ke4ls9x0PkcZ"
      },
      "source": [
        "**Exploring Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTVuRozqvrA3",
        "outputId": "eabfaec0-6e0f-448c-9b22-0ebbcb5f8820"
      },
      "outputs": [],
      "source": [
        "# check the first few rows of the data\n",
        "print(\"The first few rows:\\n\")\n",
        "print(data.head())\n",
        "print(\"\\n\")\n",
        "\n",
        "# retrive the information of the data\n",
        "print(\"Data Information:\\n\")\n",
        "print(data.info())\n",
        "print(\"\\n\")\n",
        "\n",
        "# retrive basic statistics about the data\n",
        "print(\"Data Statistics:\\n\")\n",
        "print(data.describe())\n",
        "print(\"\\n\")\n",
        "\n",
        "# retrive the shape of the data\n",
        "print(\"Data Shape:\\n\")\n",
        "print(data.shape)\n",
        "print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cERkTjIlSqqN"
      },
      "source": [
        "**Data Cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rtShkKEUxng",
        "outputId": "3dcc1b23-1647-4df7-936c-71173f46cc9e"
      },
      "outputs": [],
      "source": [
        "# the columns 8 and 21 are of type objects: string\n",
        "# the values of these columns will be checked, \n",
        "# looking for unexpected values that lead to having mixed data types\n",
        "\n",
        "# column 8\n",
        "print(\"Column 8: \")\n",
        "print(cleaned_data['jet_1_phi'].unique())\n",
        "# result: \n",
        "  # 1. float64 numeric values in string objects\n",
        "  # 2. float64 numeric values in string objects and stored in a string object\n",
        "\n",
        "# solution:\n",
        "cleaned_data['jet_1_phi']=pd.to_numeric(cleaned_data['jet_1_phi'],errors='coerce')\n",
        "print(cleaned_data['jet_1_phi'].unique().size)\n",
        "\n",
        "# column 21\n",
        "print(\"Column 21: \")\n",
        "print(cleaned_data['jet_4_btag'].unique())\n",
        "# result:\n",
        "  # 1. float64 numeric values in string objects instead of float64 type\n",
        "  # 2. alphabetical values in string objects\n",
        "\n",
        "#solution:\n",
        "cleaned_data['jet_4_btag']=pd.to_numeric(cleaned_data['jet_4_btag'], errors='coerce')\n",
        "print(cleaned_data['jet_4_btag'].unique().size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7x7hKsArStu0",
        "outputId": "720e6abf-25a1-4970-c256-83c941206543"
      },
      "outputs": [],
      "source": [
        "# remove the training examples with NaN values from the dataset\n",
        "cleaned_data.dropna(inplace=True)\n",
        "print(cleaned_data.info())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1mPLiM27fGh"
      },
      "source": [
        "For **regression and decision trees**, we will use the complete set of features (low-level and high-level combined) to take advantage of the manually constructed high-level features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYxpgKmx7fGq"
      },
      "source": [
        "**test_size** determines the proportion of the data that will be allocated for the testing set. In this case, test_size=0.2 means that **20%** of the data will be used **for testing**, and the remaining **80%** will be used for **training**.\n",
        "\n",
        "**random_state** is an optional parameter that sets the random seed used by the random number generator. This **ensures** that the **random splitting of the data is reproducible**, meaning that if you run the same code multiple times with the same random_state value, you will get the same split of data into training and testing sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_lJZOmmFUEk"
      },
      "source": [
        "**Usage of all Features: Low-Level and High-Level**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujOzI5y57fGu"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X = cleaned_data.iloc[:, 1:]  # Select all columns except the first one as features\n",
        "y = cleaned_data.iloc[:, 0]   # Select the first column as the target variable\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sOyLySTFeHE"
      },
      "source": [
        "**Usage of High-Level Features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "to5P5CCnFdBj"
      },
      "outputs": [],
      "source": [
        "# # Select the last 7 columns as high-features\n",
        "# X = cleaned_data.iloc[:, 22:]\n",
        "\n",
        "# # Select the first column as the target variable\n",
        "# y = cleaned_data.iloc[:, 0]\n",
        "\n",
        "# # Split the data into training and testing sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TX0nNtK7fGz"
      },
      "source": [
        "**Linear Regression Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LyEIlbGvv32",
        "outputId": "91fe7aea-67c6-422d-dc90-5cd0b73d433f"
      },
      "outputs": [],
      "source": [
        "# Create a linear regression model and fit it to the training set\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Use the model to make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's mean squared error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print('Mean Squared Error: {:.2f}'.format(mse))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biBeI21l7fG5"
      },
      "source": [
        "**Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJ_3iODhzFJv",
        "outputId": "f6257cfe-a316-4c5f-8f4b-3aca4f17ab90"
      },
      "outputs": [],
      "source": [
        "# Create a logistic regression model and fit it to the training set\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Use the model to make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "accuracy_logistic = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy: {:.2f}'.format(accuracy_logistic))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8hfeNIE7fG-"
      },
      "source": [
        "**Decision Trees**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTkhdAXex1vA",
        "outputId": "a9402ec3-f526-4857-c1c4-94e8c14942d0"
      },
      "outputs": [],
      "source": [
        "# Create a decision tree classifier and fit it to the training set\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the classifier's accuracy on the test set\n",
        "accuracy_decisionTrees = clf.score(X_test, y_test)\n",
        "print('Accuracy: {:.2f}'.format(accuracy_decisionTrees))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H5kV4hgEA4s"
      },
      "source": [
        "**XGBoost**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMoNjzreFFoJ"
      },
      "source": [
        "**n_estimators:** This parameter sets the number of trees to be built in the ensemble. Increasing the number of trees can improve the model's accuracy, but it can also make the model slower to train and predict.\n",
        "\n",
        "**max_depth:** This parameter sets the maximum depth of each tree in the ensemble. Increasing the maximum depth can improve the model's accuracy, but it can also make the model more prone to overfitting.\n",
        "\n",
        "**learning_rate:** This parameter sets the step size shrinkage used to prevent overfitting. Lower values of learning rate can reduce the model's accuracy, but it can also make the model more stable and less prone to overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GM3o6jQEHQ3",
        "outputId": "b5aa324f-cad3-4512-c12a-45d366b7ab83"
      },
      "outputs": [],
      "source": [
        "# Define the XGB model\n",
        "xgb_model = xgb.XGBClassifier(n_estimators=150, max_depth=7, learning_rate=0.5)\n",
        "\n",
        "# Train the model on the training data\n",
        "xgb_model.fit(X_train, y_train)\n",
        "# Make predictions on the testing data\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "accuracy_xgb = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy: {:.2f}'.format(accuracy_xgb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0nHpILG7fHF"
      },
      "source": [
        "**Neural Network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNtHEiop7fHH"
      },
      "source": [
        "According to the paper, for the *Higgs boson benchmarks*, we can use a neural network to classify the data by training it on a set of input features and corresponding labels. For the Higgs boson benchmark, we can use *either* the **low-level features**, the **high-level features**, **or the complete set of features** (low-level and high-level combined) as **inputs** to the neural network.\n",
        "\n",
        "The paper mentions that standard techniques in high-energy physics data analyses include **feed-forward neural networks with a single hidden layer**, which is an example of a *traditional shallow network*.\n",
        "\n",
        "However, recent advances in deep-learning techniques may lift the limitations of shallow networks by automatically discovering powerful nonlinear feature combinations and providing better discrimination power than current classifiers.\n",
        "\n",
        "To train a neural network, we need to choose its architecture (number of layers and units per layer), activation function, and other hyperparameters such as learning rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-97Aj_uc7fHM"
      },
      "source": [
        "**The Rectified Linear Unit (ReLU)** activation function is a commonly used activation function in neural networks. It is a simple function that returns the input if it is positive, and zero otherwise. In mathematical notation, the ReLU function is defined as:\n",
        "\n",
        "f(x) = max(0, x)\n",
        "\n",
        "where x is the input to the function and f(x) is the output.\n",
        "\n",
        "The ReLU activation function is known for its **simplicity and effectiveness** in training deep neural networks. It has been shown to **improve the speed and accuracy of the training process**, as well as help to prevent the vanishing gradient problem that can occur with other activation functions such as the sigmoid function.\n",
        "\n",
        "**Momentum** is a technique used in the *optimization of gradient descent* algorithms to *accelerate the convergence* of the training process.\n",
        "**Momentum** adds a fraction of the previous update vector to the current update vector, which helps to smooth the optimization trajectory and avoid getting stuck in local minima.\n",
        "\n",
        "**Validation split**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8levd8J7fHX"
      },
      "source": [
        "**SGD** is an optimization algorithm used to minimize the loss function during training. In contrast to batch gradient descent, which computes the gradient of the loss function using the entire dataset, SGD updates the model parameters using only a small subset of the data at a time. This makes it computationally efficient and able to handle large datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKUI0HJf7fHO",
        "outputId": "35da24e6-d082-4245-e9db-fa70a54ed9f2"
      },
      "outputs": [],
      "source": [
        "batch = 32\n",
        "act = 'relu'\n",
        "neurons = 90\n",
        "nb_epoch = 50\n",
        "# Define the neural network architecture\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(100, activation=act, input_dim=X_train.shape[1], kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.1)),\n",
        "    tf.keras.layers.Dense(75, activation=act, kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05)),\n",
        "    tf.keras.layers.Dense(50, activation=act, kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05)),\n",
        "    tf.keras.layers.Dense(25, activation=act, kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05)),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.001))\n",
        "])\n",
        "\n",
        "# Compile the model with specified hyperparameters\n",
        "opt = tf.keras.optimizers.SGD(learning_rate=0.003, momentum=0.9)\n",
        "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# create a callback to save the model with the highest accuracy\n",
        "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max')\n",
        "\n",
        "# Train the model and evaluate on a validation set\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=batch, epochs=nb_epoch, verbose=1, callbacks=[checkpoint], validation_split=0.0)\n",
        "\n",
        "# load the weights of the best model\n",
        "model.load_weights('best_model.h5')\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = [1 if y >= 0.5 else 0 for y in y_pred]  # convert probabilities to binary predictions\n",
        "\n",
        "# calculate the accuracy \n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy: {:.4f}'.format(accuracy))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
