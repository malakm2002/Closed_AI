{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2yNCR504X1Al"
      },
      "outputs": [],
      "source": [
        "# needed librairies\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import accuracy_score, roc_curve\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UKkmkmlO3v2"
      },
      "source": [
        "**Loading Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VltNZxjiO8n6",
        "outputId": "4453d9fb-309e-4872-c9b9-34a5420e107a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\tamer\\AppData\\Local\\Temp\\ipykernel_31500\\237186723.py:1: DtypeWarning: Columns (8,21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  data = pd.read_csv('HIGGS_train.csv', dtype={'8': float, '21': float})\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv('HIGGS_train.csv', dtype={'8': float, '21': float})\n",
        "\n",
        "# define the headers of the dataset\n",
        "column_names = ['class_label', 'lepton_pt', 'lepton_eta', 'lepton_phi', 'missing_energy_magnitude', 'missing_energy_phi',\n",
        "                'jet_1_pt', 'jet_1_eta', 'jet_1_phi', 'jet_1_btag', 'jet_2_pt', 'jet_2_eta', 'jet_2_phi', 'jet_2_btag',\n",
        "                'jet_3_pt', 'jet_3_eta', 'jet_3_phi', 'jet_3_btag', 'jet_4_pt', 'jet_4_eta', 'jet_4_phi', 'jet_4_btag',\n",
        "                'm_jj', 'm_jjj', 'm_lv', 'm_jlv', 'm_bb', 'm_wbb', 'm_wwbb']\n",
        "\n",
        "# assign the headers to the data\n",
        "data.columns=column_names\n",
        "\n",
        "cleaned_data = data.copy();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ke4ls9x0PkcZ"
      },
      "source": [
        "**Exploring Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTVuRozqvrA3",
        "outputId": "4373fa44-56f9-46e4-bcfc-ffc15d136c8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The first few rows:\n",
            "\n",
            "   class_label  lepton_pt  lepton_eta  lepton_phi  missing_energy_magnitude   \n",
            "0          1.0      0.908       0.329     0.35900                     1.500  \\\n",
            "1          1.0      0.799       1.470    -1.64000                     0.454   \n",
            "2          0.0      1.340      -0.877     0.93600                     1.990   \n",
            "3          1.0      1.110       0.321     1.52000                     0.883   \n",
            "4          0.0      1.600      -0.608     0.00707                     1.820   \n",
            "\n",
            "   missing_energy_phi  jet_1_pt  jet_1_eta jet_1_phi  jet_1_btag  ...   \n",
            "0              -0.313     1.100     -0.558     -1.59        2.17  ...  \\\n",
            "1               0.426     1.100      1.280      1.38        0.00  ...   \n",
            "2               0.882     1.790     -1.650    -0.942        0.00  ...   \n",
            "3              -1.210     0.681     -1.070    -0.922        0.00  ...   \n",
            "4              -0.112     0.848     -0.566      1.58        2.17  ...   \n",
            "\n",
            "   jet_4_eta  jet_4_phi  jet_4_btag   m_jj  m_jjj   m_lv  m_jlv   m_bb  m_wbb   \n",
            "0     -1.140  -0.000819         0.0  0.302  0.833  0.986  0.978  0.780  0.992  \\\n",
            "1      1.130   0.900000         0.0  0.910  1.110  0.986  0.951  0.803  0.866   \n",
            "2     -0.678  -1.360000         0.0  0.947  1.030  0.999  0.728  0.869  1.030   \n",
            "3     -0.374   0.113000         0.0  0.756  1.360  0.987  0.838  1.130  0.872   \n",
            "4     -0.654  -1.270000         3.1  0.824  0.938  0.972  0.789  0.431  0.961   \n",
            "\n",
            "   m_wwbb  \n",
            "0   0.798  \n",
            "1   0.780  \n",
            "2   0.958  \n",
            "3   0.808  \n",
            "4   0.958  \n",
            "\n",
            "[5 rows x 29 columns]\n",
            "\n",
            "\n",
            "Data Information:\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 599999 entries, 0 to 599998\n",
            "Data columns (total 29 columns):\n",
            " #   Column                    Non-Null Count   Dtype  \n",
            "---  ------                    --------------   -----  \n",
            " 0   class_label               599999 non-null  float64\n",
            " 1   lepton_pt                 599999 non-null  float64\n",
            " 2   lepton_eta                599999 non-null  float64\n",
            " 3   lepton_phi                599999 non-null  float64\n",
            " 4   missing_energy_magnitude  599999 non-null  float64\n",
            " 5   missing_energy_phi        599999 non-null  float64\n",
            " 6   jet_1_pt                  599999 non-null  float64\n",
            " 7   jet_1_eta                 599999 non-null  float64\n",
            " 8   jet_1_phi                 599999 non-null  object \n",
            " 9   jet_1_btag                599999 non-null  float64\n",
            " 10  jet_2_pt                  599999 non-null  float64\n",
            " 11  jet_2_eta                 599999 non-null  float64\n",
            " 12  jet_2_phi                 599999 non-null  float64\n",
            " 13  jet_2_btag                599999 non-null  float64\n",
            " 14  jet_3_pt                  599999 non-null  float64\n",
            " 15  jet_3_eta                 599999 non-null  float64\n",
            " 16  jet_3_phi                 599999 non-null  float64\n",
            " 17  jet_3_btag                599998 non-null  float64\n",
            " 18  jet_4_pt                  599999 non-null  float64\n",
            " 19  jet_4_eta                 599999 non-null  float64\n",
            " 20  jet_4_phi                 599999 non-null  float64\n",
            " 21  jet_4_btag                599999 non-null  object \n",
            " 22  m_jj                      599999 non-null  float64\n",
            " 23  m_jjj                     599999 non-null  float64\n",
            " 24  m_lv                      599999 non-null  float64\n",
            " 25  m_jlv                     599999 non-null  float64\n",
            " 26  m_bb                      599999 non-null  float64\n",
            " 27  m_wbb                     599999 non-null  float64\n",
            " 28  m_wwbb                    599999 non-null  float64\n",
            "dtypes: float64(27), object(2)\n",
            "memory usage: 132.8+ MB\n",
            "None\n",
            "\n",
            "\n",
            "Data Statistics:\n",
            "\n",
            "         class_label      lepton_pt     lepton_eta     lepton_phi   \n",
            "count  599999.000000  599999.000000  599999.000000  599999.000000  \\\n",
            "mean        0.529286       0.992486      -0.000111       0.000166   \n",
            "std         0.499142       0.565045       1.007858       1.005480   \n",
            "min         0.000000       0.275000      -2.430000      -1.740000   \n",
            "25%         0.000000       0.591000      -0.737000      -0.870000   \n",
            "50%         1.000000       0.854000      -0.001030       0.002640   \n",
            "75%         1.000000       1.240000       0.738000       0.870000   \n",
            "max         1.000000       8.710000       2.430000       1.740000   \n",
            "\n",
            "       missing_energy_magnitude  missing_energy_phi       jet_1_pt   \n",
            "count             599999.000000       599999.000000  599999.000000  \\\n",
            "mean                   0.998020           -0.001155       0.990147   \n",
            "std                    0.599282            1.006755       0.474626   \n",
            "min                    0.000626           -1.740000       0.139000   \n",
            "25%                    0.577000           -0.873000       0.679000   \n",
            "50%                    0.891000           -0.001920       0.894000   \n",
            "75%                    1.290000            0.872000       1.170000   \n",
            "max                    9.900000            1.740000       8.380000   \n",
            "\n",
            "           jet_1_eta     jet_1_btag       jet_2_pt  ...       jet_4_pt   \n",
            "count  599999.000000  599999.000000  599999.000000  ...  599999.000000  \\\n",
            "mean       -0.002192       1.000382       0.992592  ...       0.986216   \n",
            "std         1.010297       1.026463       0.500731  ...       0.505671   \n",
            "min        -2.970000       0.000000       0.189000  ...       0.365000   \n",
            "25%        -0.689000       0.000000       0.656000  ...       0.618000   \n",
            "50%        -0.003000       1.090000       0.890000  ...       0.868000   \n",
            "75%         0.685000       2.170000       1.200000  ...       1.220000   \n",
            "max         2.970000       2.170000      11.600000  ...      11.600000   \n",
            "\n",
            "           jet_4_eta      jet_4_phi           m_jj          m_jjj   \n",
            "count  599999.000000  599999.000000  599999.000000  599999.000000  \\\n",
            "mean       -0.000308      -0.002128       1.034047       1.024418   \n",
            "std         1.008151       1.005564       0.669432       0.378086   \n",
            "min        -2.500000      -1.740000       0.107000       0.245000   \n",
            "25%        -0.715000      -0.872000       0.791000       0.846000   \n",
            "50%        -0.000461      -0.005810       0.895000       0.950000   \n",
            "75%         0.715000       0.868000       1.020000       1.080000   \n",
            "max         2.500000       1.740000      22.300000      11.600000   \n",
            "\n",
            "                m_lv          m_jlv           m_bb          m_wbb   \n",
            "count  599999.000000  599999.000000  599999.000000  599999.000000  \\\n",
            "mean        1.050635       1.010130       0.973342       1.033155   \n",
            "std         0.164443       0.398453       0.524886       0.364497   \n",
            "min         0.092200       0.157000       0.048100       0.303000   \n",
            "25%         0.986000       0.768000       0.674000       0.819000   \n",
            "50%         0.990000       0.917000       0.873000       0.947000   \n",
            "75%         1.020000       1.140000       1.140000       1.140000   \n",
            "max         5.920000      10.500000      13.700000       8.430000   \n",
            "\n",
            "              m_wwbb  \n",
            "count  599999.000000  \n",
            "mean        0.959835  \n",
            "std         0.313074  \n",
            "min         0.351000  \n",
            "25%         0.770000  \n",
            "50%         0.872000  \n",
            "75%         1.060000  \n",
            "max         6.260000  \n",
            "\n",
            "[8 rows x 27 columns]\n",
            "\n",
            "\n",
            "Data Shape:\n",
            "\n",
            "(599999, 29)\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# check the first few rows of the data\n",
        "print(\"The first few rows:\\n\")\n",
        "print(data.head())\n",
        "print(\"\\n\")\n",
        "\n",
        "# retrive the information of the data\n",
        "print(\"Data Information:\\n\")\n",
        "print(data.info())\n",
        "print(\"\\n\")\n",
        "\n",
        "# retrive basic statistics about the data\n",
        "print(\"Data Statistics:\\n\")\n",
        "print(data.describe())\n",
        "print(\"\\n\")\n",
        "\n",
        "# retrive the shape of the data\n",
        "print(\"Data Shape:\\n\")\n",
        "print(data.shape)\n",
        "print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rtShkKEUxng",
        "outputId": "3343dcc1-1486-4703-d6cb-807c3fc8cfe5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Column 8: \n",
            "[-1.59 1.38 -0.942 ... '3.50E-01' '-6.59E-03' '\"1.01\"']\n",
            "2312\n",
            "Column 21: \n",
            "[0.0 3.1 1.55 '0.00E+00' '3.10E+00' '1.55E+00' 'error' 's']\n",
            "4\n"
          ]
        }
      ],
      "source": [
        "# the columns 8 and 21 are of type objects: string\n",
        "# the values of these columns will be checked, \n",
        "# looking for unexpected values that lead to having mixed data types\n",
        "\n",
        "# column 8\n",
        "print(\"Column 8: \")\n",
        "print(cleaned_data['jet_1_phi'].unique())\n",
        "# result: \n",
        "  # 1. float64 numeric values in string objects\n",
        "  # 2. float64 numeric values in string objects and stored in a string object\n",
        "\n",
        "# solution:\n",
        "cleaned_data['jet_1_phi']=pd.to_numeric(cleaned_data['jet_1_phi'],errors='coerce')\n",
        "print(cleaned_data['jet_1_phi'].unique().size)\n",
        "\n",
        "# column 21\n",
        "print(\"Column 21: \")\n",
        "print(cleaned_data['jet_4_btag'].unique())\n",
        "# result:\n",
        "  # 1. float64 numeric values in string objects instead of float64 type\n",
        "  # 2. alphabetical values in string objects\n",
        "\n",
        "#solution:\n",
        "cleaned_data['jet_4_btag']=pd.to_numeric(cleaned_data['jet_4_btag'], errors='coerce')\n",
        "print(cleaned_data['jet_4_btag'].unique().size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cERkTjIlSqqN"
      },
      "source": [
        "**Data Cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7x7hKsArStu0"
      },
      "outputs": [],
      "source": [
        "# remove the training examples with NaN values from the dataset\n",
        "cleaned_data.dropna(inplace=True)\n",
        "\n",
        "# testing the realtime\n",
        "\n",
        "# the cleaned dataset will be saved to a new CSV file\n",
        "cleaned_data.to_csv('HIGGS_train_cleaned.csv', index=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For **regression and decision trees**, we will use the complete set of features (low-level and high-level combined) to take advantage of the manually constructed high-level features."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Usage of the complete set of features**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**test_size** determines the proportion of the data that will be allocated for the testing set. In this case, test_size=0.2 means that **20%** of the data will be used **for testing**, and the remaining **80%** will be used for **training**.\n",
        "\n",
        "**random_state** is an optional parameter that sets the random seed used by the random number generator. This **ensures** that the **random splitting of the data is reproducible**, meaning that if you run the same code multiple times with the same random_state value, you will get the same split of data into training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X = cleaned_data.iloc[:, 1:]  # Select all columns except the first one as features\n",
        "y = cleaned_data.iloc[:, 0]   # Select the first column as the target variable\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Linear Regression Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3LyEIlbGvv32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Squared Error: 0.22\n"
          ]
        }
      ],
      "source": [
        "# Create a linear regression model and fit it to the training set\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Use the model to make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's mean squared error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print('Mean Squared Error: {:.2f}'.format(mse))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TJ_3iODhzFJv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.64\n"
          ]
        }
      ],
      "source": [
        "# Create a logistic regression model and fit it to the training set\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Use the model to make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "accuracy_logistic = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy: {:.2f}'.format(accuracy_logistic))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Decision Trees**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "KTkhdAXex1vA",
        "outputId": "0a10b4b8-f34e-41fd-c1f2-febcbfa45019"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.64\n"
          ]
        }
      ],
      "source": [
        "# Create a decision tree classifier and fit it to the training set\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the classifier's accuracy on the test set\n",
        "accuracy_decisionTrees = clf.score(X_test, y_test)\n",
        "print('Accuracy: {:.2f}'.format(accuracy_decisionTrees))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Neural Network**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "According to the paper, for the *Higgs boson benchmarks*, we can use a neural network to classify the data by training it on a set of input features and corresponding labels. For the Higgs boson benchmark, we can use *either* the **low-level features**, the **high-level features**, **or the complete set of features** (low-level and high-level combined) as **inputs** to the neural network.\n",
        "\n",
        "The paper mentions that standard techniques in high-energy physics data analyses include **feed-forward neural networks with a single hidden layer**, which is an example of a *traditional shallow network*.\n",
        "\n",
        "However, recent advances in deep-learning techniques may lift the limitations of shallow networks by automatically discovering powerful nonlinear feature combinations and providing better discrimination power than current classifiers.\n",
        "\n",
        "To train a neural network, we need to choose its architecture (number of layers and units per layer), activation function, and other hyperparameters such as learning rate and regularization. The paper provides some details about the hyperparameters used for training deep neural networks on the Higgs boson. Once the network is trained, we can use it to make predictions on new data by feeding the input features through the network and computing the output probabilities."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "According to the paper, for training deep neural networks on the Higgs boson, a **five-layer** neural network with **300 hidden units in each layer** was selected, with a **learning rate of 0.05**, and a weight decay coefficient of 1 × 10 −5.\n",
        "\n",
        "Pre-training, extra hidden units, and additional hidden layers significantly increased training time without noticeably increasing performance.\n",
        "\n",
        "Hidden units all used the **tanh activation function**. Weights were initialized from a normal distribution with zero mean and standard deviation 0.1 in the first layer, 0.001 in the output layer, and 0.05 in all other hidden layers. Gradient computations were made on mini-batches of size 100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load the low-level features\n",
        "X = cleaned_data.iloc[:, 2:23] # Select columns 2 to 22 as features: the 21 low-level features\n",
        "y = cleaned_data.iloc[:, 0]    # Select the first column as the target variable\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Tanh**, short for hyperbolic tangent, is a commonly used activation function in neural networks. It is a nonlinear function that **maps the input values to a range between -1 and 1**. The tanh function is defined mathematically as:\n",
        "\n",
        "tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n",
        "\n",
        "Like other activation functions, tanh is used to introduce nonlinearity to the output of a neuron in a neural network. It is often used in the hidden layers of neural networks as it allows the network to **capture more complex patterns in the data**.\n",
        "\n",
        "**Momentum** is a technique used in the *optimization of gradient descent* algorithms to *accelerate the convergence* of the training process.\n",
        "**Momentum** adds a fraction of the previous update vector to the current update vector, which helps to smooth the optimization trajectory and avoid getting stuck in local minima."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "4800/4800 [==============================] - 25s 5ms/step - loss: 0.6735 - accuracy: 0.5776 - val_loss: 0.6646 - val_accuracy: 0.5950\n",
            "Epoch 2/200\n",
            "4800/4800 [==============================] - 26s 5ms/step - loss: 0.6566 - accuracy: 0.6063 - val_loss: 0.6482 - val_accuracy: 0.6162\n",
            "Epoch 3/200\n",
            "4800/4800 [==============================] - 24s 5ms/step - loss: 0.6454 - accuracy: 0.6205 - val_loss: 0.6405 - val_accuracy: 0.6261\n",
            "Epoch 4/200\n",
            "4800/4800 [==============================] - 23s 5ms/step - loss: 0.6373 - accuracy: 0.6310 - val_loss: 0.6341 - val_accuracy: 0.6351\n",
            "Epoch 5/200\n",
            "4800/4800 [==============================] - 24s 5ms/step - loss: 0.6312 - accuracy: 0.6392 - val_loss: 0.6286 - val_accuracy: 0.6436\n",
            "Epoch 6/200\n",
            "4800/4800 [==============================] - 24s 5ms/step - loss: 0.6257 - accuracy: 0.6454 - val_loss: 0.6264 - val_accuracy: 0.6482\n",
            "Epoch 7/200\n",
            "4800/4800 [==============================] - 24s 5ms/step - loss: 0.6210 - accuracy: 0.6511 - val_loss: 0.6216 - val_accuracy: 0.6525\n",
            "Epoch 8/200\n",
            "4800/4800 [==============================] - 24s 5ms/step - loss: 0.6166 - accuracy: 0.6558 - val_loss: 0.6173 - val_accuracy: 0.6566\n",
            "Epoch 9/200\n",
            "4800/4800 [==============================] - 25s 5ms/step - loss: 0.6125 - accuracy: 0.6607 - val_loss: 0.6194 - val_accuracy: 0.6519\n",
            "Epoch 10/200\n",
            "4800/4800 [==============================] - 24s 5ms/step - loss: 0.6082 - accuracy: 0.6656 - val_loss: 0.6127 - val_accuracy: 0.6619\n",
            "Epoch 11/200\n",
            "4800/4800 [==============================] - 23s 5ms/step - loss: 0.6051 - accuracy: 0.6690 - val_loss: 0.6116 - val_accuracy: 0.6629\n",
            "Epoch 12/200\n",
            "4800/4800 [==============================] - 24s 5ms/step - loss: 0.6014 - accuracy: 0.6726 - val_loss: 0.6081 - val_accuracy: 0.6669\n",
            "Epoch 13/200\n",
            "4800/4800 [==============================] - 24s 5ms/step - loss: 0.5989 - accuracy: 0.6757 - val_loss: 0.6059 - val_accuracy: 0.6676\n",
            "Epoch 14/200\n",
            "4800/4800 [==============================] - 23s 5ms/step - loss: 0.5954 - accuracy: 0.6790 - val_loss: 0.6024 - val_accuracy: 0.6707\n",
            "Epoch 15/200\n",
            "4800/4800 [==============================] - 23s 5ms/step - loss: 0.5924 - accuracy: 0.6811 - val_loss: 0.6120 - val_accuracy: 0.6709\n",
            "Epoch 16/200\n",
            "4800/4800 [==============================] - 24s 5ms/step - loss: 0.5895 - accuracy: 0.6844 - val_loss: 0.6042 - val_accuracy: 0.6726\n",
            "Epoch 17/200\n",
            "4800/4800 [==============================] - 24s 5ms/step - loss: 0.5874 - accuracy: 0.6858 - val_loss: 0.6020 - val_accuracy: 0.6733\n",
            "Epoch 18/200\n",
            "4800/4800 [==============================] - 24s 5ms/step - loss: 0.5846 - accuracy: 0.6887 - val_loss: 0.6015 - val_accuracy: 0.6743\n",
            "Epoch 19/200\n",
            "4800/4800 [==============================] - 24s 5ms/step - loss: 0.5814 - accuracy: 0.6914 - val_loss: 0.5988 - val_accuracy: 0.6747\n",
            "Epoch 20/200\n",
            "4800/4800 [==============================] - 24s 5ms/step - loss: 0.5795 - accuracy: 0.6932 - val_loss: 0.5986 - val_accuracy: 0.6754\n",
            "Epoch 21/200\n",
            "4800/4800 [==============================] - 25s 5ms/step - loss: 0.5774 - accuracy: 0.6953 - val_loss: 0.5985 - val_accuracy: 0.6771\n",
            "Epoch 22/200\n",
            "4800/4800 [==============================] - 25s 5ms/step - loss: 0.5753 - accuracy: 0.6969 - val_loss: 0.5997 - val_accuracy: 0.6760\n",
            "Epoch 23/200\n",
            "4800/4800 [==============================] - 24s 5ms/step - loss: 0.5730 - accuracy: 0.6993 - val_loss: 0.5977 - val_accuracy: 0.6769\n",
            "Epoch 24/200\n",
            "4800/4800 [==============================] - 23s 5ms/step - loss: 0.5707 - accuracy: 0.7008 - val_loss: 0.6028 - val_accuracy: 0.6766\n",
            "Epoch 25/200\n",
            "4800/4800 [==============================] - 23s 5ms/step - loss: 0.5684 - accuracy: 0.7025 - val_loss: 0.6097 - val_accuracy: 0.6742\n",
            "Epoch 26/200\n",
            "4800/4800 [==============================] - 23s 5ms/step - loss: 0.5663 - accuracy: 0.7044 - val_loss: 0.5997 - val_accuracy: 0.6757\n",
            "Epoch 27/200\n",
            "4800/4800 [==============================] - 24s 5ms/step - loss: 0.5642 - accuracy: 0.7060 - val_loss: 0.5992 - val_accuracy: 0.6786\n",
            "Epoch 28/200\n",
            "4800/4800 [==============================] - 24s 5ms/step - loss: 0.5622 - accuracy: 0.7083 - val_loss: 0.6032 - val_accuracy: 0.6767\n",
            "Epoch 29/200\n",
            "4800/4800 [==============================] - 26s 5ms/step - loss: 0.5600 - accuracy: 0.7097 - val_loss: 0.6132 - val_accuracy: 0.6752\n",
            "Epoch 30/200\n",
            "4800/4800 [==============================] - 24s 5ms/step - loss: 0.5580 - accuracy: 0.7117 - val_loss: 0.5996 - val_accuracy: 0.6750\n",
            "Epoch 31/200\n",
            "4800/4800 [==============================] - 23s 5ms/step - loss: 0.5557 - accuracy: 0.7131 - val_loss: 0.5999 - val_accuracy: 0.6780\n",
            "Epoch 32/200\n",
            "4800/4800 [==============================] - 23s 5ms/step - loss: 0.5540 - accuracy: 0.7143 - val_loss: 0.6093 - val_accuracy: 0.6781\n",
            "Epoch 33/200\n",
            "4800/4800 [==============================] - 22s 5ms/step - loss: 0.5522 - accuracy: 0.7160 - val_loss: 0.6007 - val_accuracy: 0.6745\n",
            "Epoch 34/200\n",
            "4800/4800 [==============================] - 22s 5ms/step - loss: 0.5501 - accuracy: 0.7175 - val_loss: 0.6181 - val_accuracy: 0.6742\n",
            "Epoch 35/200\n",
            "4800/4800 [==============================] - 23s 5ms/step - loss: 0.5480 - accuracy: 0.7192 - val_loss: 0.6089 - val_accuracy: 0.6726\n",
            "Epoch 36/200\n",
            "4800/4800 [==============================] - 23s 5ms/step - loss: 0.5460 - accuracy: 0.7208 - val_loss: 0.6134 - val_accuracy: 0.6733\n",
            "Epoch 37/200\n",
            "4800/4800 [==============================] - 24s 5ms/step - loss: 0.5436 - accuracy: 0.7224 - val_loss: 0.6032 - val_accuracy: 0.6751\n",
            "Epoch 38/200\n",
            "4800/4800 [==============================] - 24s 5ms/step - loss: 0.5420 - accuracy: 0.7235 - val_loss: 0.6021 - val_accuracy: 0.6744\n",
            "Epoch 39/200\n",
            "4800/4800 [==============================] - 25s 5ms/step - loss: 0.5410 - accuracy: 0.7252 - val_loss: 0.6032 - val_accuracy: 0.6735\n",
            "Epoch 40/200\n",
            "4800/4800 [==============================] - 25s 5ms/step - loss: 0.5387 - accuracy: 0.7265 - val_loss: 0.6230 - val_accuracy: 0.6725\n",
            "Epoch 41/200\n",
            "4800/4800 [==============================] - 26s 5ms/step - loss: 0.5372 - accuracy: 0.7278 - val_loss: 0.6157 - val_accuracy: 0.6725\n",
            "Epoch 42/200\n",
            "4800/4800 [==============================] - 25s 5ms/step - loss: 0.5347 - accuracy: 0.7295 - val_loss: 0.6158 - val_accuracy: 0.6744\n",
            "Epoch 43/200\n",
            "4800/4800 [==============================] - 24s 5ms/step - loss: 0.5333 - accuracy: 0.7308 - val_loss: 0.6494 - val_accuracy: 0.6733\n",
            "Epoch 44/200\n",
            "4800/4800 [==============================] - 25s 5ms/step - loss: 0.5319 - accuracy: 0.7318 - val_loss: 0.6355 - val_accuracy: 0.6729\n",
            "Epoch 45/200\n",
            "4800/4800 [==============================] - 25s 5ms/step - loss: 0.5298 - accuracy: 0.7330 - val_loss: 0.6153 - val_accuracy: 0.6666\n",
            "Epoch 46/200\n",
            "4800/4800 [==============================] - 23s 5ms/step - loss: 0.5290 - accuracy: 0.7338 - val_loss: 0.6116 - val_accuracy: 0.6715\n",
            "Epoch 47/200\n",
            "4800/4800 [==============================] - 23s 5ms/step - loss: 0.5271 - accuracy: 0.7355 - val_loss: 0.6169 - val_accuracy: 0.6693\n",
            "Epoch 48/200\n",
            "4800/4800 [==============================] - 24s 5ms/step - loss: 0.5254 - accuracy: 0.7366 - val_loss: 0.6098 - val_accuracy: 0.6704\n",
            "Epoch 49/200\n",
            "4800/4800 [==============================] - 23s 5ms/step - loss: 0.5243 - accuracy: 0.7373 - val_loss: 0.6178 - val_accuracy: 0.6653\n",
            "Epoch 50/200\n",
            "4800/4800 [==============================] - 23s 5ms/step - loss: 0.5226 - accuracy: 0.7396 - val_loss: 0.6221 - val_accuracy: 0.6627\n",
            "Epoch 51/200\n",
            "4800/4800 [==============================] - 23s 5ms/step - loss: 0.5211 - accuracy: 0.7398 - val_loss: 0.6174 - val_accuracy: 0.6695\n",
            "Epoch 52/200\n",
            "4800/4800 [==============================] - 23s 5ms/step - loss: 0.5198 - accuracy: 0.7410 - val_loss: 0.6156 - val_accuracy: 0.6673\n",
            "Epoch 53/200\n",
            "4800/4800 [==============================] - 175s 37ms/step - loss: 0.5181 - accuracy: 0.7424 - val_loss: 0.6495 - val_accuracy: 0.6664\n",
            "Epoch 54/200\n",
            "4800/4800 [==============================] - 19s 4ms/step - loss: 0.5163 - accuracy: 0.7432 - val_loss: 0.6268 - val_accuracy: 0.6690\n",
            "Epoch 55/200\n",
            "4800/4800 [==============================] - 22s 5ms/step - loss: 0.5151 - accuracy: 0.7443 - val_loss: 0.6528 - val_accuracy: 0.6622\n",
            "Epoch 56/200\n",
            "4800/4800 [==============================] - 23s 5ms/step - loss: 0.5135 - accuracy: 0.7450 - val_loss: 0.6615 - val_accuracy: 0.6650\n",
            "Epoch 57/200\n",
            "4800/4800 [==============================] - 19s 4ms/step - loss: 0.5128 - accuracy: 0.7464 - val_loss: 0.6323 - val_accuracy: 0.6655\n",
            "Epoch 58/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.5112 - accuracy: 0.7471 - val_loss: 0.6286 - val_accuracy: 0.6649\n",
            "Epoch 59/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.5109 - accuracy: 0.7471 - val_loss: 0.6265 - val_accuracy: 0.6660\n",
            "Epoch 60/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.5101 - accuracy: 0.7477 - val_loss: 0.6507 - val_accuracy: 0.6648\n",
            "Epoch 61/200\n",
            "4800/4800 [==============================] - 13s 3ms/step - loss: 0.5079 - accuracy: 0.7496 - val_loss: 0.6544 - val_accuracy: 0.6618\n",
            "Epoch 62/200\n",
            "4800/4800 [==============================] - 13s 3ms/step - loss: 0.5073 - accuracy: 0.7497 - val_loss: 0.6373 - val_accuracy: 0.6619\n",
            "Epoch 63/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.5051 - accuracy: 0.7514 - val_loss: 0.6498 - val_accuracy: 0.6613\n",
            "Epoch 64/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.5045 - accuracy: 0.7522 - val_loss: 0.6509 - val_accuracy: 0.6631\n",
            "Epoch 65/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.5038 - accuracy: 0.7527 - val_loss: 0.6394 - val_accuracy: 0.6634\n",
            "Epoch 66/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.5020 - accuracy: 0.7534 - val_loss: 0.6732 - val_accuracy: 0.6602\n",
            "Epoch 67/200\n",
            "4800/4800 [==============================] - 13s 3ms/step - loss: 0.5015 - accuracy: 0.7544 - val_loss: 0.6643 - val_accuracy: 0.6638\n",
            "Epoch 68/200\n",
            "4800/4800 [==============================] - 13s 3ms/step - loss: 0.5008 - accuracy: 0.7549 - val_loss: 0.6689 - val_accuracy: 0.6628\n",
            "Epoch 69/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4988 - accuracy: 0.7557 - val_loss: 0.6630 - val_accuracy: 0.6608\n",
            "Epoch 70/200\n",
            "4800/4800 [==============================] - 13s 3ms/step - loss: 0.4987 - accuracy: 0.7557 - val_loss: 0.6434 - val_accuracy: 0.6618\n",
            "Epoch 71/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4974 - accuracy: 0.7573 - val_loss: 0.6408 - val_accuracy: 0.6622\n",
            "Epoch 72/200\n",
            "4800/4800 [==============================] - 13s 3ms/step - loss: 0.4967 - accuracy: 0.7574 - val_loss: 0.6206 - val_accuracy: 0.6609\n",
            "Epoch 73/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4952 - accuracy: 0.7580 - val_loss: 0.6530 - val_accuracy: 0.6615\n",
            "Epoch 74/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4950 - accuracy: 0.7588 - val_loss: 0.6497 - val_accuracy: 0.6604\n",
            "Epoch 75/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4941 - accuracy: 0.7590 - val_loss: 0.6480 - val_accuracy: 0.6589\n",
            "Epoch 76/200\n",
            "4800/4800 [==============================] - 13s 3ms/step - loss: 0.4922 - accuracy: 0.7602 - val_loss: 0.6794 - val_accuracy: 0.6597\n",
            "Epoch 77/200\n",
            "4800/4800 [==============================] - 13s 3ms/step - loss: 0.4915 - accuracy: 0.7606 - val_loss: 0.6319 - val_accuracy: 0.6588\n",
            "Epoch 78/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4922 - accuracy: 0.7606 - val_loss: 0.6348 - val_accuracy: 0.6616\n",
            "Epoch 79/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4911 - accuracy: 0.7611 - val_loss: 0.6226 - val_accuracy: 0.6609\n",
            "Epoch 80/200\n",
            "4800/4800 [==============================] - 13s 3ms/step - loss: 0.4902 - accuracy: 0.7615 - val_loss: 0.6923 - val_accuracy: 0.6538\n",
            "Epoch 81/200\n",
            "4800/4800 [==============================] - 13s 3ms/step - loss: 0.4907 - accuracy: 0.7614 - val_loss: 0.6820 - val_accuracy: 0.6571\n",
            "Epoch 82/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4893 - accuracy: 0.7624 - val_loss: 0.6755 - val_accuracy: 0.6554\n",
            "Epoch 83/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4885 - accuracy: 0.7628 - val_loss: 0.6331 - val_accuracy: 0.6568\n",
            "Epoch 84/200\n",
            "4800/4800 [==============================] - 15s 3ms/step - loss: 0.4872 - accuracy: 0.7636 - val_loss: 0.6974 - val_accuracy: 0.6575\n",
            "Epoch 85/200\n",
            "4800/4800 [==============================] - 17s 4ms/step - loss: 0.4873 - accuracy: 0.7641 - val_loss: 0.6769 - val_accuracy: 0.6538\n",
            "Epoch 86/200\n",
            "4800/4800 [==============================] - 22s 5ms/step - loss: 0.4868 - accuracy: 0.7639 - val_loss: 0.6753 - val_accuracy: 0.6562\n",
            "Epoch 87/200\n",
            "4800/4800 [==============================] - 22s 5ms/step - loss: 0.4862 - accuracy: 0.7642 - val_loss: 0.6893 - val_accuracy: 0.6573\n",
            "Epoch 88/200\n",
            "4800/4800 [==============================] - 22s 5ms/step - loss: 0.4866 - accuracy: 0.7642 - val_loss: 0.6570 - val_accuracy: 0.6546\n",
            "Epoch 89/200\n",
            "4800/4800 [==============================] - 22s 5ms/step - loss: 0.4863 - accuracy: 0.7645 - val_loss: 0.6459 - val_accuracy: 0.6538\n",
            "Epoch 90/200\n",
            "4800/4800 [==============================] - 22s 5ms/step - loss: 0.4852 - accuracy: 0.7648 - val_loss: 0.6515 - val_accuracy: 0.6551\n",
            "Epoch 91/200\n",
            "4800/4800 [==============================] - 22s 5ms/step - loss: 0.4848 - accuracy: 0.7656 - val_loss: 0.6933 - val_accuracy: 0.6534\n",
            "Epoch 92/200\n",
            "4800/4800 [==============================] - 15s 3ms/step - loss: 0.4839 - accuracy: 0.7667 - val_loss: 0.6560 - val_accuracy: 0.6558\n",
            "Epoch 93/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4833 - accuracy: 0.7660 - val_loss: 0.6520 - val_accuracy: 0.6571\n",
            "Epoch 94/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4817 - accuracy: 0.7678 - val_loss: 0.6617 - val_accuracy: 0.6559\n",
            "Epoch 95/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4826 - accuracy: 0.7669 - val_loss: 0.6799 - val_accuracy: 0.6550\n",
            "Epoch 96/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4824 - accuracy: 0.7669 - val_loss: 0.6526 - val_accuracy: 0.6583\n",
            "Epoch 97/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4823 - accuracy: 0.7673 - val_loss: 0.6507 - val_accuracy: 0.6574\n",
            "Epoch 98/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4825 - accuracy: 0.7679 - val_loss: 0.7348 - val_accuracy: 0.6537\n",
            "Epoch 99/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4823 - accuracy: 0.7673 - val_loss: 0.6842 - val_accuracy: 0.6548\n",
            "Epoch 100/200\n",
            "4800/4800 [==============================] - 17s 4ms/step - loss: 0.4812 - accuracy: 0.7679 - val_loss: 0.6674 - val_accuracy: 0.6547\n",
            "Epoch 101/200\n",
            "4800/4800 [==============================] - 23s 5ms/step - loss: 0.4802 - accuracy: 0.7684 - val_loss: 0.6494 - val_accuracy: 0.6567\n",
            "Epoch 102/200\n",
            "4800/4800 [==============================] - 23s 5ms/step - loss: 0.4811 - accuracy: 0.7677 - val_loss: 0.6629 - val_accuracy: 0.6571\n",
            "Epoch 103/200\n",
            "4800/4800 [==============================] - 23s 5ms/step - loss: 0.4790 - accuracy: 0.7690 - val_loss: 0.6893 - val_accuracy: 0.6495\n",
            "Epoch 104/200\n",
            "4800/4800 [==============================] - 23s 5ms/step - loss: 0.4796 - accuracy: 0.7690 - val_loss: 0.6774 - val_accuracy: 0.6531\n",
            "Epoch 105/200\n",
            "4800/4800 [==============================] - 24s 5ms/step - loss: 0.4806 - accuracy: 0.7685 - val_loss: 0.6983 - val_accuracy: 0.6552\n",
            "Epoch 106/200\n",
            "4800/4800 [==============================] - 23s 5ms/step - loss: 0.4789 - accuracy: 0.7695 - val_loss: 0.6493 - val_accuracy: 0.6558\n",
            "Epoch 107/200\n",
            "4800/4800 [==============================] - 23s 5ms/step - loss: 0.4788 - accuracy: 0.7698 - val_loss: 0.6931 - val_accuracy: 0.6511\n",
            "Epoch 108/200\n",
            "4800/4800 [==============================] - 24s 5ms/step - loss: 0.4776 - accuracy: 0.7706 - val_loss: 0.7095 - val_accuracy: 0.6509\n",
            "Epoch 109/200\n",
            "4800/4800 [==============================] - 23s 5ms/step - loss: 0.4793 - accuracy: 0.7696 - val_loss: 0.6562 - val_accuracy: 0.6552\n",
            "Epoch 110/200\n",
            "4800/4800 [==============================] - 23s 5ms/step - loss: 0.4780 - accuracy: 0.7700 - val_loss: 0.6736 - val_accuracy: 0.6533\n",
            "Epoch 111/200\n",
            "4800/4800 [==============================] - 23s 5ms/step - loss: 0.4772 - accuracy: 0.7711 - val_loss: 0.6644 - val_accuracy: 0.6514\n",
            "Epoch 112/200\n",
            "4800/4800 [==============================] - 24s 5ms/step - loss: 0.4770 - accuracy: 0.7711 - val_loss: 0.6333 - val_accuracy: 0.6556\n",
            "Epoch 113/200\n",
            "4800/4800 [==============================] - 24s 5ms/step - loss: 0.4789 - accuracy: 0.7700 - val_loss: 0.7032 - val_accuracy: 0.6512\n",
            "Epoch 114/200\n",
            "4800/4800 [==============================] - 17s 4ms/step - loss: 0.4794 - accuracy: 0.7691 - val_loss: 0.7343 - val_accuracy: 0.6503\n",
            "Epoch 115/200\n",
            "4800/4800 [==============================] - 13s 3ms/step - loss: 0.4774 - accuracy: 0.7706 - val_loss: 0.6698 - val_accuracy: 0.6463\n",
            "Epoch 116/200\n",
            "4800/4800 [==============================] - 13s 3ms/step - loss: 0.4778 - accuracy: 0.7705 - val_loss: 0.6983 - val_accuracy: 0.6542\n",
            "Epoch 117/200\n",
            "4800/4800 [==============================] - 13s 3ms/step - loss: 0.4773 - accuracy: 0.7706 - val_loss: 0.6628 - val_accuracy: 0.6538\n",
            "Epoch 118/200\n",
            "4800/4800 [==============================] - 13s 3ms/step - loss: 0.4766 - accuracy: 0.7713 - val_loss: 0.6441 - val_accuracy: 0.6537\n",
            "Epoch 119/200\n",
            "4800/4800 [==============================] - 13s 3ms/step - loss: 0.4767 - accuracy: 0.7710 - val_loss: 0.6651 - val_accuracy: 0.6539\n",
            "Epoch 120/200\n",
            "4800/4800 [==============================] - 13s 3ms/step - loss: 0.4768 - accuracy: 0.7710 - val_loss: 0.7145 - val_accuracy: 0.6534\n",
            "Epoch 121/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4766 - accuracy: 0.7716 - val_loss: 0.6863 - val_accuracy: 0.6501\n",
            "Epoch 122/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4772 - accuracy: 0.7707 - val_loss: 0.6440 - val_accuracy: 0.6520\n",
            "Epoch 123/200\n",
            "4800/4800 [==============================] - 19s 4ms/step - loss: 0.4758 - accuracy: 0.7717 - val_loss: 0.6724 - val_accuracy: 0.6537\n",
            "Epoch 124/200\n",
            "4800/4800 [==============================] - 23s 5ms/step - loss: 0.4786 - accuracy: 0.7700 - val_loss: 0.6661 - val_accuracy: 0.6538\n",
            "Epoch 125/200\n",
            "4800/4800 [==============================] - 23s 5ms/step - loss: 0.4775 - accuracy: 0.7707 - val_loss: 0.6879 - val_accuracy: 0.6475\n",
            "Epoch 126/200\n",
            "4800/4800 [==============================] - 23s 5ms/step - loss: 0.4763 - accuracy: 0.7717 - val_loss: 0.7057 - val_accuracy: 0.6504\n",
            "Epoch 127/200\n",
            "4800/4800 [==============================] - 18s 4ms/step - loss: 0.4762 - accuracy: 0.7715 - val_loss: 0.6911 - val_accuracy: 0.6502\n",
            "Epoch 128/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4759 - accuracy: 0.7717 - val_loss: 0.7267 - val_accuracy: 0.6498\n",
            "Epoch 129/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4771 - accuracy: 0.7709 - val_loss: 0.7433 - val_accuracy: 0.6458\n",
            "Epoch 130/200\n",
            "4800/4800 [==============================] - 13s 3ms/step - loss: 0.4766 - accuracy: 0.7720 - val_loss: 0.7124 - val_accuracy: 0.6506\n",
            "Epoch 131/200\n",
            "4800/4800 [==============================] - 13s 3ms/step - loss: 0.4760 - accuracy: 0.7721 - val_loss: 0.6659 - val_accuracy: 0.6524\n",
            "Epoch 132/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4773 - accuracy: 0.7710 - val_loss: 0.6947 - val_accuracy: 0.6520\n",
            "Epoch 133/200\n",
            "4800/4800 [==============================] - 15s 3ms/step - loss: 0.4761 - accuracy: 0.7717 - val_loss: 0.6591 - val_accuracy: 0.6489\n",
            "Epoch 134/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4746 - accuracy: 0.7730 - val_loss: 0.6912 - val_accuracy: 0.6516\n",
            "Epoch 135/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4759 - accuracy: 0.7711 - val_loss: 0.6563 - val_accuracy: 0.6473\n",
            "Epoch 136/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4767 - accuracy: 0.7704 - val_loss: 0.6752 - val_accuracy: 0.6556\n",
            "Epoch 137/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4763 - accuracy: 0.7717 - val_loss: 0.7027 - val_accuracy: 0.6536\n",
            "Epoch 138/200\n",
            "4800/4800 [==============================] - 13s 3ms/step - loss: 0.4756 - accuracy: 0.7716 - val_loss: 0.7258 - val_accuracy: 0.6475\n",
            "Epoch 139/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4756 - accuracy: 0.7727 - val_loss: 0.7244 - val_accuracy: 0.6515\n",
            "Epoch 140/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4745 - accuracy: 0.7730 - val_loss: 0.7155 - val_accuracy: 0.6522\n",
            "Epoch 141/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4745 - accuracy: 0.7725 - val_loss: 0.7244 - val_accuracy: 0.6505\n",
            "Epoch 142/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4751 - accuracy: 0.7718 - val_loss: 0.6449 - val_accuracy: 0.6519\n",
            "Epoch 143/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4741 - accuracy: 0.7728 - val_loss: 0.6827 - val_accuracy: 0.6501\n",
            "Epoch 144/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4771 - accuracy: 0.7708 - val_loss: 0.6601 - val_accuracy: 0.6475\n",
            "Epoch 145/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4772 - accuracy: 0.7712 - val_loss: 0.6792 - val_accuracy: 0.6504\n",
            "Epoch 146/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4748 - accuracy: 0.7723 - val_loss: 0.6677 - val_accuracy: 0.6452\n",
            "Epoch 147/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4761 - accuracy: 0.7707 - val_loss: 0.6653 - val_accuracy: 0.6492\n",
            "Epoch 148/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4758 - accuracy: 0.7723 - val_loss: 0.6438 - val_accuracy: 0.6500\n",
            "Epoch 149/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4759 - accuracy: 0.7712 - val_loss: 0.7012 - val_accuracy: 0.6466\n",
            "Epoch 150/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4742 - accuracy: 0.7717 - val_loss: 0.6516 - val_accuracy: 0.6509\n",
            "Epoch 151/200\n",
            "4800/4800 [==============================] - 13s 3ms/step - loss: 0.4759 - accuracy: 0.7714 - val_loss: 0.7000 - val_accuracy: 0.6468\n",
            "Epoch 152/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4761 - accuracy: 0.7720 - val_loss: 0.6722 - val_accuracy: 0.6519\n",
            "Epoch 153/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4775 - accuracy: 0.7701 - val_loss: 0.6536 - val_accuracy: 0.6517\n",
            "Epoch 154/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4770 - accuracy: 0.7707 - val_loss: 0.6920 - val_accuracy: 0.6496\n",
            "Epoch 155/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4775 - accuracy: 0.7703 - val_loss: 0.6725 - val_accuracy: 0.6525\n",
            "Epoch 156/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4773 - accuracy: 0.7707 - val_loss: 0.7088 - val_accuracy: 0.6495\n",
            "Epoch 157/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4757 - accuracy: 0.7713 - val_loss: 0.7030 - val_accuracy: 0.6488\n",
            "Epoch 158/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4763 - accuracy: 0.7719 - val_loss: 0.7183 - val_accuracy: 0.6505\n",
            "Epoch 159/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4764 - accuracy: 0.7712 - val_loss: 0.6802 - val_accuracy: 0.6466\n",
            "Epoch 160/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4762 - accuracy: 0.7718 - val_loss: 0.6605 - val_accuracy: 0.6511\n",
            "Epoch 161/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4764 - accuracy: 0.7714 - val_loss: 0.7132 - val_accuracy: 0.6497\n",
            "Epoch 162/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4758 - accuracy: 0.7716 - val_loss: 0.7162 - val_accuracy: 0.6515\n",
            "Epoch 163/200\n",
            "4800/4800 [==============================] - 13s 3ms/step - loss: 0.4770 - accuracy: 0.7703 - val_loss: 0.7041 - val_accuracy: 0.6505\n",
            "Epoch 164/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4768 - accuracy: 0.7712 - val_loss: 0.6767 - val_accuracy: 0.6532\n",
            "Epoch 165/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4765 - accuracy: 0.7714 - val_loss: 0.6958 - val_accuracy: 0.6478\n",
            "Epoch 166/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4764 - accuracy: 0.7722 - val_loss: 0.6630 - val_accuracy: 0.6481\n",
            "Epoch 167/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4775 - accuracy: 0.7700 - val_loss: 0.7162 - val_accuracy: 0.6545\n",
            "Epoch 168/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4772 - accuracy: 0.7708 - val_loss: 0.6636 - val_accuracy: 0.6522\n",
            "Epoch 169/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4780 - accuracy: 0.7704 - val_loss: 0.6812 - val_accuracy: 0.6499\n",
            "Epoch 170/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4799 - accuracy: 0.7682 - val_loss: 0.7031 - val_accuracy: 0.6470\n",
            "Epoch 171/200\n",
            "4800/4800 [==============================] - 13s 3ms/step - loss: 0.4797 - accuracy: 0.7686 - val_loss: 0.6905 - val_accuracy: 0.6515\n",
            "Epoch 172/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4792 - accuracy: 0.7691 - val_loss: 0.7058 - val_accuracy: 0.6480\n",
            "Epoch 173/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4802 - accuracy: 0.7689 - val_loss: 0.6840 - val_accuracy: 0.6486\n",
            "Epoch 174/200\n",
            "4800/4800 [==============================] - 13s 3ms/step - loss: 0.4818 - accuracy: 0.7679 - val_loss: 0.6721 - val_accuracy: 0.6454\n",
            "Epoch 175/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4823 - accuracy: 0.7675 - val_loss: 0.7352 - val_accuracy: 0.6509\n",
            "Epoch 176/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4805 - accuracy: 0.7684 - val_loss: 0.6829 - val_accuracy: 0.6502\n",
            "Epoch 177/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4813 - accuracy: 0.7685 - val_loss: 0.7003 - val_accuracy: 0.6501\n",
            "Epoch 178/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4806 - accuracy: 0.7685 - val_loss: 0.7062 - val_accuracy: 0.6476\n",
            "Epoch 179/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4797 - accuracy: 0.7693 - val_loss: 0.6757 - val_accuracy: 0.6430\n",
            "Epoch 180/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4803 - accuracy: 0.7683 - val_loss: 0.6681 - val_accuracy: 0.6445\n",
            "Epoch 181/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4803 - accuracy: 0.7688 - val_loss: 0.6691 - val_accuracy: 0.6496\n",
            "Epoch 182/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4827 - accuracy: 0.7670 - val_loss: 0.6831 - val_accuracy: 0.6492\n",
            "Epoch 183/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4820 - accuracy: 0.7672 - val_loss: 0.6637 - val_accuracy: 0.6511\n",
            "Epoch 184/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4817 - accuracy: 0.7675 - val_loss: 0.6955 - val_accuracy: 0.6529\n",
            "Epoch 185/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4837 - accuracy: 0.7666 - val_loss: 0.6594 - val_accuracy: 0.6444\n",
            "Epoch 186/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4816 - accuracy: 0.7676 - val_loss: 0.7516 - val_accuracy: 0.6483\n",
            "Epoch 187/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4829 - accuracy: 0.7673 - val_loss: 0.7012 - val_accuracy: 0.6482\n",
            "Epoch 188/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4824 - accuracy: 0.7677 - val_loss: 0.6903 - val_accuracy: 0.6507\n",
            "Epoch 189/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4845 - accuracy: 0.7664 - val_loss: 0.6655 - val_accuracy: 0.6471\n",
            "Epoch 190/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4842 - accuracy: 0.7658 - val_loss: 0.7108 - val_accuracy: 0.6496\n",
            "Epoch 191/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4846 - accuracy: 0.7659 - val_loss: 0.6949 - val_accuracy: 0.6443\n",
            "Epoch 192/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4856 - accuracy: 0.7653 - val_loss: 0.7050 - val_accuracy: 0.6470\n",
            "Epoch 193/200\n",
            "4800/4800 [==============================] - 13s 3ms/step - loss: 0.4859 - accuracy: 0.7649 - val_loss: 0.6686 - val_accuracy: 0.6501\n",
            "Epoch 194/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4844 - accuracy: 0.7658 - val_loss: 0.6527 - val_accuracy: 0.6508\n",
            "Epoch 195/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4845 - accuracy: 0.7655 - val_loss: 0.7017 - val_accuracy: 0.6481\n",
            "Epoch 196/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4847 - accuracy: 0.7654 - val_loss: 0.7102 - val_accuracy: 0.6481\n",
            "Epoch 197/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4852 - accuracy: 0.7658 - val_loss: 0.6685 - val_accuracy: 0.6451\n",
            "Epoch 198/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4872 - accuracy: 0.7640 - val_loss: 0.6710 - val_accuracy: 0.6459\n",
            "Epoch 199/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4866 - accuracy: 0.7643 - val_loss: 0.6810 - val_accuracy: 0.6455\n",
            "Epoch 200/200\n",
            "4800/4800 [==============================] - 14s 3ms/step - loss: 0.4885 - accuracy: 0.7634 - val_loss: 0.6767 - val_accuracy: 0.6489\n",
            "3750/3750 [==============================] - 4s 973us/step\n",
            "Accuracy: 0.65\n"
          ]
        }
      ],
      "source": [
        "# Define the neural network architecture\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(300, activation='tanh', input_dim=X_train.shape[1], kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.1)),\n",
        "    tf.keras.layers.Dense(300, activation='tanh', kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05)),\n",
        "    tf.keras.layers.Dense(300, activation='tanh', kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05)),\n",
        "    tf.keras.layers.Dense(300, activation='tanh', kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05)),\n",
        "    tf.keras.layers.Dense(300, activation='tanh', kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05)),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.001))\n",
        "])\n",
        "\n",
        "# Compile the model with specified hyperparameters\n",
        "opt = tf.keras.optimizers.SGD(learning_rate=0.05, momentum=0.9)\n",
        "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=100, epochs=200, verbose=1)\n",
        "\n",
        "# evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = [1 if y >= 0.5 else 0 for y in y_pred]  # convert probabilities to binary predictions\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy: {:.2f}'.format(accuracy))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The decrease in the mean and standard deviation of the weight initialization is to prevent exploding gradients as we move forward towards the output layer during training."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "According to the paper, the primary metric for comparison of deep neural networks in high-energy physics is the area under the receiver operating characteristic curve (**AUC**), with **larger** AUC values indicating **higher** **classification** **accuracy** across a range of threshold choices. For example, for the Higgs benchmark, deep neural networks achieved an AUC of 0.88 on both the low-level and complete feature sets when trained with dropout."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
